---
title: "Getting and Cleaning Data Coursera Notes"
author: "Nathan Young"
output: "pdf_document"
---

# Week 1
## Obtaining Data Motivation
### Video Notes
This course covers basic ideas behind getting data ready for analysis. 

- Finding and extracting raw data
- Tidy data principles and how to make data tiny
- Practical implemenetation through a range of R packages

What you'd desire is a nice format like an excel spreadsheet. 
More commonly will get not nice data that requires parsing.  
Can also get data in other formats like JSON. JSON is easy to send, but not to analyze.  
Might need to extract from text files.  
Might need to extract from databases such as mySQL, MongoDB.  
Might be stored online or on computer such as on twitter API.  

Pipeline: 
Raw data -> Processing Script -> tidy data -> data analysis -> data communication
Courses tend to scip raw through tidy data. 

## Raw and Processed Data
How can raw data be different depending on who you talk to?  
Definition of data: "values of qualitative or quantitative variables, belonging to a set of items."  
**Raw data:**

* original source of data
* often hard to use for data analysis, hard to parse
* Data analysis *includes* processing
* raw data may only need to be processed once (keep a record of what you did)

**Processed Data:**

* Data that is ready for analysis
* can include merging, subsetting, transforming, etc. 
* may be standards for processing 
* all steps should be recorded!

Processing pipeline example: An ilumina sequencing machine
Starts with fragments of DNA bound to slide, chemical process that copies sequence, color of image changes in regions and this change is analyzed, producing a fast Q file where each segment of the data produces a sequence of letters. Any of these stages can be considered raw. How the raw data goes to tidy data can influence downstream analysis and it is important to understand what happened in the process so your analysis doesn't contain artifacts.  

## Components of Tidy Data
The target to get to from raw data. 
Should have 4 things after going from raw to tidy:

1. The raw data
2. a tidy data set
3. a code book describing each variable and its value in the tidy data set
4. An explicit and exact recipe you used to go from 1 -> 2 & 3

Your R script can be the recipe for step 4. 

**Raw Data**

* The strange binary file your measurement machine spits out
* An unformatted Excel file with 10 worksheets the company you contracted with sent you
* The complicated JSON data you got from scraping the Twitter API
* The hand-entered numbers you collected looking through a microscope

*You know the raw data is in the right format if you:*

1. Ran no software on the data
2. Did not manipulate any of the numbers in the data
3. You did not remove any data from the dataset
4. You did not summarize the data in any way

**The tidy data**

1. Each variable you measure should be in one column
2. Each different observation should be in a different row
3. Should be one table for each "kind" of variable
4. If you have multiple tables, should include a column in table that allows them to be linked together. 

*Other important tips*

* Include a row at top of each file with variable names
* Make variable names human readable (AgeAtDiagnosis instead of AgeDx)
* In general, data should be saved in one file per table

**The Code Book**
Should include:

1. Information about the variables (and units!) in the data set not contained in the tidy data. 
2. Information about the summary choices you made
3. Information about experimental study design you used

*Other Important tips*

* A common format for this document is a Word/text file
* There should be a section called "Study Design" that has a thorough description of how you collected the data 
* There must be a section called "Code Book" that descibes variables and units

**The Instruction List**

* Ideally a computer script (In R or Python)
* Input for the script is the raw data
* Output is processed tidy data
* No parameters to the script (end user doesn't need to input anything)

In some cases it will not be possible to script every step. In that case you should provide instructions like: 

1. Step 1 - take the raw file, run version 3.1.2 of summarize software with parameters a = 1, b = 2, c = 3
2. Step 2 - Run software separately for each sample
3. Step 3 - Take column three of outputfile.out for each sample and that is the corresponding row in the output data set 

When in doubt, include more information.  

##Downloading Files
In era of internet, most files will be from internet. Why use R to download? The information will then be included in script to support above initiatives.  
Get and set working directory. 

* A basic component of working with data is knowing your working directory
* The two main components are `getwd()` and `setwd()`
* Be aware of relative vs absolute paths
  + Relative: `setwd("./data"), setwd("../")`
  + ../ moves up one level
  + Absolute: `setwd("/Users/jtleek/data/")`
* Important difference in Windows: `setwd("C:\\Users\\Andrew\\Downloads")
  + In windows, need to use forward slashes
  
First step is to check for and create directories

* file.exists("DirectoryName") will check to see if the directory exists
* dir.create("directoryName") will create a directory if it doesn't exist
* Example of checking for a "data" directory and creating one if it doesn't exist. 

```{r, eval = FALSE}
if(!file.exists("data")) {
  dire.create("data")
}
```
### Get data from internet: download.file()

* downloads from the internet
* even if you can do this by hand, improves reproducibility
* Important parameters are *url, destfile, method*
* Useful for downloading tab-delimited, csv, and other files

Example - Baltimore fixed camera data (speed cameras around Baltimore)
```{r, eval = FALSE}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType = DOWNLOAD"
# method is important for downloading in Mac, maybe not on Windows
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")
# Would list file named "cameras.csv" in "data" folder. 
list.files("./data")
```
Good idea to include a data downloaded in file
```{r}
dateDownloaded <- date()
dateDownloaded
```
Notes about download.file()

* If url starts with *http* you can use download.file
* If url starts with *https* on Windows you are ok
* If url starts with *https* on Mac may need *method="curl"*
* If file is big, may take a while
* Be sure to record when you downloaded

## Reading Local Files
### Covered in R lectures
Download file as above and check if directory "data" exists. 
#### Loading flat files - read.table()

* This is the main function for reading data into R
* Flexible and robust but requires more parameters
* Reads data into RAM - big dataset can cause problems
* Important parameters *file, header, sep, row.names, nrows*
* Related functions read.csv() and read.csv2()

Baltimore example: 
```{r, eval = FALSE}
# Throws error because not ' ' delimited
cameraData <- read.table("./data/camera.csv")
# Change sep parameter
cameraData <- read.table("./data/cameras.csv", sep = ",", header = TRUE)
head(cameraData)
```
Can also use read.csv as this automatically sets sep = "," and head = TRUE
Other important parameters: 

* quote - you can tell R whether there are any quoted values quote = "" means no quote
* na.strings - set the character that represents a missing value
* nrows - how many rows to read of the file (nrows = 10 reads 10 rows)
* skip - number of lines to skip before starting to read

In instructor's experience: biggest problem with reading flat files are quotation marks placed in data values. Setting quote = "" often resolves these

## Reading Excel Files
May be viewed snobbishly, but still a very common way to have data saved in this format. Google spreadsheets for example. 
Baltimore camera data: 
```{r, eval = FALSE}
if(!file.exists("data")){dir.create("data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx",method="curl)
dateDownloaded<-date()
```
Need xlsx package (among others), ammend for Tidyverse
```{r, eval = FALSE}
library(xlsx)
cameraData <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,header=TRUE)
head(cameraData)
# Can subset data: col 2 & 3 and rows 1-4
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,
  colIndex=colIndex,rowIndex=rowIndex)
cameraDataSubset
```
#### Further Notes

* write.xlsx function writes out Excel file with similar arguments
* read.xlsx2 is much faster than read.xlsx but for reading subsets of rows may be slightly unstable. 
* The XLConnect package has more options for writing and manipulating Excel files
* The XLConnect vignette is a good place to start for that package
* In general, it is advised to store your data in either a database, csv, or tab separated files as they are easier to distribute. 

## Reading XML

* Extensible markup language
* Used to store structured data
* particularly widely used in internet applications
* extracting xml is basis for most web scraping
* components
  + Markup - labels that give the text structure
  + Content - actual text of the document

### Tags, elements, and attributes

* Tags correspond to general labels
  + Start tags <section>
  + End tags </section>
  + Empty tags <line-break />
* Elements are specific examples of tags
  + <Greeting> Hello, world </Greeting>
* Attributes are componenets of label
  + <img src="jeff.jpg" alt="instructor"/> (src and alt are attributes)
  + <step number="3"> Connect A to B. </step> (number is attribute)

Read the file into R
```{r, eval = FALSE}
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
```
### Directly access parts of XML document
```{r, eval = FALSE}
rootNode[[1]] # will contain food tags and information
rootNode[[1]][[1]] # Enters first element then first element of that component
```
### Programatically extract parts of the file
```{r, eval = FALSE}
xmlSApply(rootNode, xmlValue)
```
### XPath

* /node Top level node
* //node Node at any level
* node[@attr-name] Node with an attribute name
* node[@attr-name='bob'] Node with attribute name attr-name='bob'

For more: http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf

### Get the items on the menu and prices
```{r, eval = FALSE}
xpathSApply(rootNode, "//name", xmlValue) #goes through and gets all nodes that have element of title //name
xpathSApply(rootNode,"//price",xmlValue) #same, but with price
```

### Extract contents by attribute
```{r, eval = FALSE}
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl, useInternal = TRUE) #need html tree parse
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
scores
teams
```
Official xml tutorials online. And an outstanding guide to XML package
https://github.com/DataScienceSpecialization/courses/tree/master/03_GettingData/lectures

## Reading JSON

* Javascript object notation
* lightweight data storage
* common format for data from application programming interfaces (API)
* Similar structure to XML but different syntax/format
* Data stored as numbers, strings, boolean, array, object

Reading data from JSON
```{r}
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData) # get dataframe out
names(jsonData$owner) # get names of dataframe within dataframe jsonData
```
Writing data frames to JSON
```{r, eval = FALSE}
myjson <- toJSON(iris,pretty = TRUE) #pretty makes output readable
```
Can send back to dataframe with fromJSON(myjson). This is different as you can use the html address or the object myjson.  
Further reading: 

* http://www.json.org/
* Tutorial on jsonlite http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/
* and the vignette

## The data.table Package

* inherets from data.frame
  + all functions that accept data.frame work with data.table
* Written in C so much faster
* much, much faster at subsetting, grouping, updating

Has slightly new syntax
```{r}
library(data.table)
DF = data.frame(x=rnorm(9), y = rep(c("a","b","c"),each=3),z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9), y = rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)
```
See all data tables in memory: tables()
Subsetting rows
```{r}
DT[2,] # same as df
DT[DT$y == "a"] # same as df
DT[c(2,3)] # different than df, bases on rows instead of columns
DT[,c(2,3)] # Won't give what you expect! Different function
```
Column subsetting: uses expressions. Modified for data.table. Argument you pass after the comma is called an "expression". In R, an expression is a collection of statements enclosed in curley brackets.  
Calculating values for variables with expressions
```{r}
DT[,list(mean(x),sum(z))] #pass list of functions applied to variables named by columns
DT[,table(y)]
DT[,w:=z^2] # Add new column efficiently
# This doesn't copy the whole df. 
# Need to be careful with pointing to a table and editing the pointed one, changes both. 
# Need to explicitly copy the function if you want to
# Multiple operations example

# m will be assigned the last step log2
DT[,m:={tmp <- (x+z);log2(tmp+5)}]

# plyr like operations
DT[,a:=x>0]
DT[,b:=mean(x+w),by=a]
```
Special variables: 
.N: an integer, length 1, containing the number
```{r}
set.seed(123);
DT <- data.table(x=sample(letters[1:3],1E5, TRUE))
DT[, .N, by=x] # Will count instance of each letter of x, very quickly
```
Keys: subset and sort table more rapidly than with df
```{r}
DT <- data.table(x=rep(c("a","b","c"),each=100),y=rnorm(300))
setkey(DT,x)
# subset on 'x' and looks for 'a'
DT['a']
```
Joins
```{r}
DT1 <- data.table(x=c("a","a","b","dt1"),y=1:4)
DT2 <- data.table(x=c("a","b","dt2"),z=5:7)
setkey(DT1,x); setkey(DT2,x)
merge(DT1,DT2)
```
Fast reading from disk
```{r}
big_df <- data.frame(x=rnorm(1E6),y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file)) # for read.table with tab separated files
system.time(read.table(file,header=TRUE, sep="\t"))
```
Summary and Further Reading

* latest development version contains new functions like melt and dcast for data.tables
  + https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&root=datatable
* list of differences between data.table and data.frame
  + http://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table
* Notes based on https://github.com/raphg/Biostat-578/blob/master/Advanced_data_manipulation.Rpres

# Week 2
## Reading from MySQL
What is mySQL? It is a free and widely used open source database software for internet based applications. Data is structured in databases, tables within databases, and fields (columns) within tables. Each row is called a record. Check out wikipedia for more info on structure.  
An example structure are tables containing info on a company with departments, managers, employees, salaries, titles, etc. Each of these contain data that are linked together and essentially each one is a dataframe.  
The first step to using the mySQL package is to install mySQL at http://dev.mysql.com/doc/refman/5.7/en/installing.html where there are links to each operating system you could want to install on. 
Before installing mySQL, need to install microsoft visuall c++ redistributible. 
Then can install RMySQL: official instructions at http://biostat.mc.vanderbilt.edu/wiki/Main/RMySQL  

Note: as a data scientist, you are usually handed a database and trying to get data out of it rather than creating a database. 

Note: I haven't installed the packages yet! 
```{r}
library(RMySQL)
ucscDB <- dbConnect(MySQL(), user = "genome",
  host = "genome-mysql.cse.ucsc.edu")
# note the "show databases;" is not an R command but a SQL
result <- dbGetQuery(ucscDB, "show databases;");dbDisconnect(ucscDB);
# Connect to particular database hg19
hg19 <- dbConnect(MySQL(), user="genome", db="hg19",
  host="genome-mysql.cse.ucsc.edu")
# Find number of tables in it. 
allTables <- dbListTables(hg19)
length(allTables) #10949
# Show column names within a particular table
dbListFields(hg19,"affyU133Plus2")
# Find number of records (rows) in table by sending query to db
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
# Get one table out as a data frame
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
dbDisconnect(hg19)
length(allTables) #10949
```
Within the database, thee are a bunch of tables. Each table contains information about a particular item so you can list the fields of the dataset with `dbListFields`.  
`dbReadTable` to read information from database into data.frame.  
Often a huge amount of data stored in a database so you can select subsets of data such as with `query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3:); fetch(query, n=10)` will only select 10 elements.  
Full queries of mySQL are beyond scope of this course but you can find them in the MySQL documentation. 
Further Resources: 

* RMySQL vignette: http://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf
* List of commands: http://www.pantz.org/software/mysql/mysqlcommands.html
  + In general, be careful with mysql commands
* Nice blog post summarizing: http://www.r-bloggers.com/mysql-and-r/


## Reading Data from HDF5

* Used for storing large data sets
* Supports storing a range of data types
* Heirarchical data format
* *groups* containing zero or more data sets and metadata
  + Have a *group header* with group name and list of attributes
  + Have a *group symbol table* with a list of objects in a group
* *datasets* multidimensional arrays of data elements with metadata
  + Have a *header* with name, datatype, dataspace, and storage layout
  + Have a *data array* with the data, akin to dataframe

### R HDF5 package
sourced from biocLite.R
```{r, eval = FALSE}
library(rhdf5)
# For this lecture, we will create an exmample file. 
created = h5createFile("example.h5")
created

# Create groups within file
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa") #subgroup of foo called foobaa
h5ls("example.h5")

# Write to groups
A = matrix(1:10, nr = 5, nc = 2)
h5write(A, "example.h5", "foo/A")
B = array(seq(0.1, 2.0, by=0.1), dim = c(5, 2, 2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")

# Write a dataset
df = data.frame(1L:5L, seq(0,1,length.out=5), c("ab","cde","fghi","a","s"),stringsAsFactors = FALSE)
h5write(df,"example.h5","df")
h5ls("example.h5")

# Read data
readA = h5read("example.h5","foo/A")
readB = h5read("example.h5","foo/foobaa/B")
readdf = h5read("example.h5","df")
readA

# Reading and writing chunks
# This will write the values 12, 13, 14 to the first three rows of column 1 of A
#h5write(c(12,13,14), "example.h5", "foo/A", index = list(1:3,1))
#h5read("example.h5","foo/A")
```
### Notes and further resources

* hdf5 can be used to optimize reading/writing from disc in R
* rhdf5 tutorial: 
  + http://www.bioconductor.org/packages/release/bioc/vignettes/rhdf5/inst/doc/rhdf5.pdf
* HDF group has information on HDF5 in general at http://www.hdfgroup.org/HDF5/

## Reading from the Web
### Webscraping
To programatically extract data from the HTML code of websites

* It can be a great way to get data
* Many websites have information you may want to programatically read
* In some case, this is against the ToS of a website
* Attempting to read too many pages too quickly can get your IP address blocked, be careful! 

Get data off webpages with readLines()
```{r}
con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode <- readLines(con)
close(con) # Close connection after reading
htmlCode # will be hard to read
```
Can help readability by parsing with XML
Note that this method may need updated. 
```{r}
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes = TRUE)
xpathSApply(html,"//title",xmlValue)
```
### GET from the httr package
This works in comparison to XML package above! 
```{r}
library(httr)
html2 <- GET("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
content2 <- content(html2, as="text") # extract content
parsedHtml <- htmlParse(content2, asText = TRUE) # parse, will look exactly same as XML package
```
If you try to access a website that requires a password, you will get a 401 error but you can authenticate with the httr package. 
```{r}
pg2 <- GET("http://httpbin.org/basic-augh/user/passwd",authenticate("user","passwd"))
pg2 # should be status 200
names(pg2)
```
### Using handles
You can assign a website a handle. Big advantage is you can then pass different paths within the website as well as not needing to reauthenticate again. 
```{r}
google = handle("http://google.com")
pg1 <- GET(handle=google, path = "/")
pg2 <- GET(handle=google, path = "search)
```
Notes and further resources: 

* R Bloggers has a number of examples of web scraping at http://www.r-bloggers.com/?s=Web+Scraping
* The httr help file has useful examples http://cran.r-project.org/web/packages/httr/httr.pdf
* See later lectures on APIs

## Reading from APIs
Application programming interfaces. Most internet companies like Twitter or Facebook will have an API where you can download data. For example, you can get data on what is tweeting.  
Usually the first thing you'll need to do is create an account with the API at the account of each organization. https://dev.twitter.com/apps
Accessing Twitter from R
```{r, eval = FALSE}
# Authenticate app
myapp <- oauth_app("twitter", key = "yourConsumerKeyHere", 
    secret = "yourConsumerSecretHere")
# Sign in
sig <- sign_oauth1.0(myapp,
    token = "yourTokenhere",
    token_secret = "yourTokenSecretHere")
# Extra things that he want's to analyze, his home timeline, will get the page contaiing the JSON data
homeTL <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",sig)
```
You'll have to convert the JSON you get as a dataframe.  

In general, look at the documentation. 

* httr allows GET, POST, PUT, and DELETE requests if you are authorized
* you can authenticate with a username or a password
* most modern APIs use something like oauth
* httr works well with facebook, google, twitter, github, etc. 

If you go to httr demo component on GitHub you can see a bunch of examples of how to access API's of different websites. 

## Reading from other sources
### There is an R package for that

* Roger has a nice video on how there are R packages for most things that you will want to access
* Interesting packages to follow
* In general, best way to find if an R package exists is to google "data storage mechanism R package" ie "MySQL R Package"

### Interacting more directly with files

* file - open a connection to a text file
* url - open a connection to a url
* gzfile - open a connection to a .gz file
* bzfile - open a connection to a .bz2 file
* ?connections for more information
* **remember to close connections**

### Foreign package

* Loads data from Minitab, S, SAS, SPSS, Stata, Systat
* basic functions are *read.foo*
  + read.arft (Weka)
  + read.dta (Stata)
  + read.mtp (Minitab)
  + read.octave (Octave)
  + read.spss (SPSS)
  + read.xport (SAS)
* See the help page for more details: http://cran.r-project.org/web/packages/foreign/foreign.pdf

### Other database packages

* RPostresSQL, RODBC, RMongo

### Reading Images

* jpeg - http://cran.r-project.org/web/packages/jpeg/index.html
* readbitmap - http://cran.r-project.org/web/packages/readbitmap/indes.html
* png - http://cran.r-project.org/web/packages/png/index.html
* EBImage (Bioconductor) - http://wwww.bioconductor.org/packages/2.13/bioc/html/EBImage.html

### Reading GIS data
Geographic information systems

* rdgal - http://cran.r-project.org/web/packages/rgdal/index.html
* rgeos - http://cran.r-project.org/web/packages/rgeos/index.html
* raster - http://cran.r-project.org/web/packages/raster/index.html

### Reading Music Data

* tuneR - http://cran.r-project.org/web/packages/tuneR/
* seewave - http://rug.mnhn.fr/seewave/

# Week 3
## Subsetting and Sorting
Once data is loaded into R, you will likely want to manipulate it to be a tidy data set. Variables in columns, observations in rows.  
Subsetting - quick review from r programming class
```{r}
set.seed(13435)
X <- data.frame("var1"=sample(1:5), "var2"=sample(6:10), "var3"=sample(11:15))
X <- X[sample(1:5),];X$var2[c(1,3)] = NA
X
# Column subset
X[,1]
# Column subset with variable name
X[,"var1"]
# Subset row and column simultaneously
# row 1 through 2 and variable "var2"
X[1:2, "var2"]

# Logicals ands and ors
# Find rows where var1 <=3 and var3 > 11 is true
X[(X$var1 <= 3 & X$var3 > 11),]
# Find rows where var1 <= 3 or var3 > 15
X[(X$var1 <= 3 | X$var3 > 15),]

# Dealing with missing values
# Subsetting on NA's will not give actual rows, use which command as
# this doesn't return NA's
X[which(X$var2 > 8),]

# Sorting
sort(X$var1)
# decreasing order
sort(X$var1, decreasing = TRUE)
# send NA's to end
sort(X$var2, na.last = TRUE)

# Order by a variable (sort rows)
X[order(X$var1),]
# Multiple variables ordered, var1 first then var3 as tiebreaker
X[order(X$var1,X$var3),]

# Can do same thing with plyr
library(plyr)
# order X by var1
arrange(X,var1)
# Decreasing
arrange(X,desc(var1))

# adding rows and columns
# Same length 
X$var4 <- rnorm(5)
X
# Use cbind instead
# can bind to either side by changing order
Y <- cbind(X,rnorm(5))
# can use rbind, order logically same. 
```

### notes and further resources

* R Programming in the Data Science Track
* Andrew Jaffe's lecture notes http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf

## Summarizing Data
Key aspect of data cleaning is loading data into R and finding quirks, weird issues, missing values, or other problems that you need to address before doing downstream analysis. 
Example from boston data set on restaurants. Note the original link in the notes doesn't work, found a copy of the dataset on GitHub. 
```{r}
library(RCurl)
if(!file.exists("./data")){dir.create("./data")}
fileURL <- getURL("https://gist.githubusercontent.com/slowteetoe/528c78213fcd80f05419/raw/e0a4a89476fca79e692df1a373e5025f5112a5f6/restaurants.csv")
download.file(fileURL, destfile = "./data/restaurants.csv",method="curl")
restData <- read.csv("./data/restaurants.csv")
# to see a reasonable amount of the dataframe, use head()
head(restData,n=3)
# to see end of datframe, use tail()
tail(restData,n=3)
# Get overall summary: 
# count for characters, summary stats for integers
summary(restData)
# str(restData) for more info
str(restData)
# variability 
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5, 0.75, 0.9))

# summary tables of specific tables to find issues
# useNA if any tells you if any missing values
table(restData$zipCode,useNA="ifany")
# 2D table, district vs zip code
table(restData$councilDistrict,restData$zipCode)

# Check for missing values
sum(is.na(restData$councilDistrict))
any(is.na(restData$councilDistrict)) # checks if any are NA
all(restData$zipCode > 0)

# Row and col sum
colSums(is.na(restData))
all(colSums(is.na(restData))

# Values with specific characteristics
table(restData$zipCode %in% c("21212"))
table(restData$zipCode %in% c("21212","21213"))
# Use logical to subset dataset
head(restData[restData$zipCode %in% c("21212","21213"),])

# Cross tabs (summaries)
data(UCBAdmissions)
DF <- as.data.frame(UCBAdmissions)
summary(df)
xt <- xtabs(Freq ~ Gender + Admit, data = DF)

# Flat tables
warpbreaks$replicate <- rep(1:9, len=54)
# * denotes all other data in dataset, would be difficult to see due to size
xt <- xtabs(breaks ~ .,data = warpbreaks)
# summarize to flat table
ftable(xt)

# Size of a data set
fakeData <- rnorm(1e5)
object.size(fakeData)
print(object.size(fakeData),units="Mb")
```

## Creating New Variables
Why create new variables? 

* Often the raw data won't have a value you are looking for
* You will need to transform the data to get the values you would like
* Usually you will add those values to the data frames you are working with
* Common variables to create
  + Missingness indicators
  + "cutting up" quantitative variables
  + Applying transforms

Example using same baltimore data set. 
```{r}
# Creating sequences, often used to index different operations on data
s1 <- seq(1,10, by=2); s1
# length alternatively
s2 <- seq(1,10, length = 3); s2
# create index along certain values
x <- c(1, 3, 8, 25, 100); seq(along = x)

# Subsetting variables
# Subset dataset to include ones that are near me
restData$nearMe <- restData$neighborhood %in% c("Roland Park","Homeland")
table(restData$nearMe)

# Create binary variables
# Check whether condition is true or false
restData$zipWrong <- ifelse(restData$zipCode < 0, TRUE, FALSE)
# check prior
table(restData$zipWrong, restData$ zipCode < 0)

# Creating categorical variables
restData$zipGroups <- cut(restData$zipCode, breaks = quantile(restData$zipCode))
table(restData$zipGroups)
# Easier cutting
library(Hmisc)
restData$zipGroups <- cut2(restData$zipCode, g=4)
table(restData$zipGroups)

# Create factor variables
restData$zcf <- factor(restData$zipCode)
restData$zcf[1:10]
class(restData$zcf)

# Levels of factor variables
# Dummy factor variables
yesno <- sample(c("yes","no"), size=10, replace = TRUE)
yesnofac <- factor(yesno,levels=c("yes","no"))
# By default, treats 'no' as first value can change as below
relevel(yesnofac,ref="yes")
as.numeric(yesnofac)

# Cutting produces factor variables

# Using mutate function
# create new variable and add to dataset
library(plyr); library(Hmisc)
restData2 <- mutate(restData, zipGroups = cut2(zipCode, g=4))
table(restData$zipGroups)
```

Common Transformations

* abs(x)
* sqrt(x)
* ceiling(x)
* floor(x)
* round(x, digits = n)
* signif(x, digits = n)
* cos(x), sin(x), etc
* log(x) natural log
* log2(x), log10(x) other logs
* exp(x)

Notes and further reading:

* Tutorial from developer of plyr - http://plyr.had.co.nz/09-user/
* Andrew Jaffe's R Notes http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf

## Reshaping Data
Data often comes in a weird format, variable in rows, observation in columns etc. May need to reshape to format. Recall goal of tidy data: 

1. Each variable forms a column
2. Each observation forms a row
3. Each table/file stores data about one kind of observation

### Start with Reshaping
```{r}
library(reshape2)
# standard R dataset
head(mtcars)

# melt data frame
mtcars$carname <- rownames(mtcars)
# takes a list of id variables and measure variables
# melted to be tall and skinny with one column of variables containing
# both mpg and hp
carMelt <- melt(mtcars,id = c("carname","gear',"cyl"), measure.vars = c("mpg","hp"))
head(carMelt,n=3)
tail(carMelt,n=3)
```
Now that it's melted, we can recast it to different shapes. 
```{r}
# Will show the cylinders broken down by the different variables
cylData <- dcast(carMelt, cyl ~ varaible)
cylData
# passing mean to dcast
cylData <- dcast(carMelt, cyl ~ variable, mean)
cylData
```
Averaging values over a particular factor
```{r}
head(InsectSprays)
tapply(InsectSprays$count, InsectSprays$spray, sum)

# Via split, apply, combine
spIns <- split(InsectSprays$count, InsectSprays$spray)
spIns
sprCount <- lapply(spIns,sum)
sprCount
unlist(sprCount)
# alternatively
sapply(spIns,sum)

# With plyr package
ddply(InsectSprays, .(spray), summarize,sum = sum(count))

spraySums <- ddply(InsectSprays,.(spray),summarize,sum=ave(count,FUN=sum))
dim(spraySums)
head(spraySums)
```
More Information

* A tutorial from the developer of plyr http://plyr.had.co.nz/09-user/
* Nice reshape tutorial http://www.slideshare.net/jeffreybreen/reshaping-data-in-r
* Good plyr primer http://www.r-bloggers.com/a-quick-primer-on-split-apply-combine-problems/
* See also the functions
  + acast - for casting as multidimensional arrays
  + arrange - for faster reordering without using order() commands
  + mutate - adding new variables

## Managin Data Frames with dplyr
Pronounced D-plier, packaged to help with dataframes
Data frame is key data structure in statistics and in R. 

* assumption in plyr: one observation per row
* assumption in plyr: each column represents a variable or measure or characteristic
* primary implementation that you will use is the default R implementation
* Can use with other implementations, particularly relational databases systems

* Developed by Hadley Wickham of RStudio
* An optimized and distilled version of plyr (also by Hadley)
* Does not provide any 'new' functionality, but *greatly* simplifies existing functionality in R
* Provides a 'grammar' for data manipulation
* Is *very* fast, many key operations are coded in C++

Verbs: 

* select: return a subset of columns of a dataframe
* filter: extract a subset of rows based on logical conditions
* arrange: reorder rows
* rename: rename variables
* mutate: add new variables (columns) or transform existing
* summarise: generage summary statistics of different variables, possibly within strata
* Handy print method to prevent from printing a lot to console. 

Properties

* First argument is a dataframe
* Susequent arguments describe what to do with it, and you can refer to columns in the df directly without using the $ operator
* Result is a df
* df must be properly formatted and annotated for this to be at all useful

## Managing df with dplyr - Basic Tools
```{r}
chicago <- readRDS("chicago.rds")
dim(chicago)
str(chicago)
names(chicago)
# look at columns by name! city to dptp
head(select(chicago, city:dptp))
# Look at all columns except
head(select(chicago, -(city:dptp))
# without dplyr, need to find indexes
i <- match("city",names(chicago))
j <- match("dptp",names(chicago))
head(chicago[, -(i:j)])

# Filter subset rows based on conditions
chic.f <- filter(chicago, pm25tmean2 > 30)
head(chic.f)
# Can do complex logical sequences
chic.f <- filter(chicago, pm25tmean > 30 & tmpd > 80)
head(chic.f)

# arange by date
chicago <- arrange(chicago, date)
head(chicago)
tail(chicago)
# reverse order
chicago arrange(chicago, desc(date))
head(chicago)
tail(chicago)

# rename, hard without dplyr
chicago <- rename(chicago, pm25=pm25mean2, dewpoint=dptp)
head(chicago)

# mutate, create new variable called 2.5detrend
chicago <- mutate(chicago, pm25detrend = pm25 - miean(pm25, na.rm = TRUE))
head(select(chicago, pm25, pm25detrend))

chicago <- mutate(chicago, tempcat = factor(1 * (tmpd > 80), labels = c("cold", "hot")))
hotcold <- group_by(chicago, tempcat)
summarize(hotcold, pm25 = mean(pm25, na.rm = TRUE), o3 = max(o3tmean2), no2 = median(no2tmean2))

chicago <- mutate(chicago, year = as.POSIXlt(date).year)
years <- group_by(chicago, year)
summarize(years, pm25 = mean(pm25, na.rm = TRUE), o3 = max(o3tmean2), no2 = median(no2tmean2))

# %>% pipeline operator 
chicago %>% mutate(month = POSIXlt(date)$mon + 1) %>% group_by(month) %>% summarize(pm25 = mean(pm25, na.rm = TRUE), o3 = mean(o3tmean2), no2 = median(no2tmean2))
# handy tool to eliminate assigning a lot of intermdiate variables
```
Once you learn dplyr "grammar" there are additional benefits. dplyr can work with other data frame "backends" such as SQL databases. data.table for fast large tables. 

## Merging Data
Sometimes you will have multiple data sets loaded into R and will want to match them according to some ID. Similar to idea of linked tables in MySQL.  
Peer review experiment data.  
```{r}
if(!file.exists("./data")){dir.create("./data")}
fileUrl1 <- "https://dl.dropboxusercontent.com/u/7710864/data/reviews-apr29.csv"
fileUrl1 <- "https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
download.file(fileUrl1, destfile = "./data/reviews.csv",method = "curl")
download.file(fileUrl2, destfile = "./data/solutions.csv",method = "curl")
reviews <- read.csv("./data/reviews.csv")
solutions <- read.csv("./data/solutions.csv")

# Both these dataframes has matching id's. 
```
Merge command has important parameters: x and y (data frames) and use by.x or by.y and will merge by all columns that have the same name. We can tell it which veriables to merge on. 
```{r}
mergedData <- merge(reviews,solutions, by.x = "solution_id", by.y = "id", all = TRUE)
head(mergedData)
```
Default - merge on all common column names
```{r}
# The common names are "id", "start", "stop", "time_left"
intersect(names(solutions), names(reviews))
```
The problem is these don't refer to the same things across both data frames so they will merge to have rows that are improperly merged.  
Can also use join in plyr package. Will be faster, but less robust. Can only merge by names with same names.  
```{r}
df1 <- data.frame(id = sample(1:10), x = rnorm(10))
df2 <- data.frame(id = sample(1:10), y = rnorm(10))
# Will join by id (randomized in above), arrange puts them in order. 
arrange(join(df1, df2), id)
```
If you have multiple df, it is difficult to do with merge. If they all have a common id though, it is easier in plyr. 
```{r}
df1 <- data.frame(id = sample(1:10), x = rnorm(10))
df2 <- data.frame(id = sample(1:10), y = rnorm(10))
df3 <- data.frame(id = sample(1:10), z = rnorm(10))
# put them all in a list, then join_all
dfList = list(df1, df2, df3)
join_all(dfList)
```
More on merging data

* QUick R data merging page: http://www.statmetods.net/management/merging.html
* plyr information: http://plyr.had.co.nz/
* Types of joins: http://en.wikipedia.org/wiki/Join_(SQL), worth paying attention to before you do a bunch of joins

# Week 4
## Editing Text Variables
A common data clearning step is to have text variables that are in sort of ASCII format, or have udnerscores, etc to remove programmatically. The baltiomore city data example on Coursera has changed so I'll copy the examples here but cannot run them in R. 
```{r, eval = FALSE}
if(!file.exists("./data")) {dir.create("./data")}
fileUrl <- "DOESN'T WORK"
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")
cameraData <- read.csv("./data/cameras.csv")
names(cameraData)
# "address" "direction" "street" "crossStreet" "Intersection" "Location.1"
# Capitalized s in crossStreet is easy to miss and error prone. 
# tolower will change them to all lowercase
tolower(names(cameraData))
```
ANother thing you might wnat to do is separate variables with things split by periods. Use string split command
```{r, eval = FALSE}
# . is reserved so you need character escape \\
splitNames = strsplit(names(cameraData), "\\.")
splitNames[[5]] #gives "intersection"
splitNmes[[6]] # gives "location" "1" 
```
Aside on lists: might want to just subset out part without names
```{r}
myList <- list(letters = c("A", "c", "C"), numbers = 1:3, matrix(1:25, ncol = 5))
mylist[1]
mylist$letters
mylist[[1]]
```

Former camera data - to programmatically select desired info, write a function
sapply() applies a function to each element of a vector or list
```{R, eval = FALSE}
splitNames[[6]][1] # Will return "Location" only
firstElement <- function(x) {x[1]}
sapply(splitNames, firstElement)
# Will return "address" "direction" "street" "crossStreet" "intersection" "Location"
```
Example from Peer Review Data, simililarly the links don't work. 
```{r}
reviews <- data.frame("id" = c(1, 2), solution_id = c(3,4), review_id = c(27, 22), start = c(1304095698, 1304095188), "stop" = c(1304095758, 1304095206), "time_left" = c(1754, 2306), "accept" = c(1,1))
solutions <- data.frame("id" = c(1, 2), problem_id = c(156, 269), subject_id = c(29, 25), start = c(1304095119, 1304095119), "stop" = c(1304095169, 1304095183), "time_left" = c(2343, 2329), "answer" = c("B","C"))
# Things to do, subset out characters
# every time you see a _, sub out with nothing, only does first underscore
sub("_","",names(reviews),)
# gsub would do all underscores. 
testName <- "this_is_a_test"
gsub("_","",testName)
```
Finding Values
```{r, eval = FALSE}
grep("Alameda", cameraData$intersection)
# Finds all intersections where Alameda is one of the roads
# Returns 4, 5, 36

# Grepl will look for string and return a T/F vector
table(grepl("Alameda", cameraData$intersection))
# WIll reeturn a table of T/F where Alameda appears
# Can also use grepl to subset data to include search parameter
cameraData2 <- cameraData[!grepl("Alameda",cameraData$intersection),]
```
More on grep: 
value = TRUE will return the whole expression where the result occurred instead of index
Can look for values that don't appear, will return integer 0 with length 0. 

More useful string functions
```{r}
library(stringr)
nchar("Jeffrey Leek")

substr("Jeffrey Leek", 1, 7)

paste("Jeffrey", "Leek")

paste0("Jeffrey", "Leek")

str_trim("Jeff     ")
```

Important points about text in data sets

* Names of variables should be
  + All lower case if possible
  + Descriptive (Diagnosis vs Dx)
  + Not duplicated
  + Not have underscores or dots or whitespaces
* Variables with character values
  + Should usually be made into factor variables (depends on application)
  + Should be descriptive (use TRUE/FALSE instead of 0/1 and Male/Female verses 0/1 or M/F)

I lost notes due to forgetting to save. There was a substantial section on regex for which a reference sheet can be found. https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf

## Regex abbreviated
Regular expressions are a combination of literals and metacharacters. Can be thought of as literal text forming words and metacharacters defining grammar. 
Metacharacters: 

* "^i think" start of line
* "morning$" end of line
* "[Bb][Uu][Ss][Hh]" set of characters (a character class)
  + combine: "^[Ii] am" set of characters at beginning
  + "[0-9]": range of characters
  + "[^?.]$" ^ inside character class is negation. Will look at end of line for anything other than a period or question mark

## Working with dates
Can be quirky. 
Starting simple: 
```{r}
d1 <- date()
d1 #current date
class(d1) # character

d2 <- Sys.Date()
d2 # current date
class(d2) # date
```
The date class will make some things easier but others harder. 
Formatting dates: 

* %d = day as number(0-31)
* %a = abbreviated workday
* %A = unabbreviated workday
* %m = month (00-12)
* %b = abbreviated month
* %B = unabbreviated month
* %y = 2 digit year
* %Y = 4 digit year

```{r}
format(d2, "%a %b %d")
```

Changing a character vector to dates
```{r}
x <- c("1jan1960:, "31mar1960", "30jul1960"); z <- as.Date(x, "%d%b%Y")
# Can now do operations on the dates!
z[1] - z[2] # Time difference of -1 days

as.numeric(z[1] - z[2])
```
Converting to Julian
```{r}
weekdays(d2)
months(d2)
julian(d2)
```
Lubridate makes dates even easier! Has functions that will look for dates in a wide variety of formats automatically. 
```{r}
library(lubridate); 
ymd("20140108")
mdy("08/04/2013")
dmy("03-04-03")
ymd_hms("2011-08-03 10:15:03")
ymd_hms("2011-08-03 10:15:03", tz = "Pacific/Auckland") # set timezone
?Sys.timezone
```
Some functions may have slightly different syntax. 
```{r}
x <- dmy(c("1jan2013", "2jan2013", "31mar2013","30jul2013"))
wkday(x[1]) # gives number
wkday(x[1], label = TRUE)
```

Notes and further resources: 

* More info on lubridate: http://www.r-statistics.com/2012/03/do-more-with-dates-and-times-in-r-with-lubridate-1-1-0/
* Lubridate vignette: http://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html
* Ultimately you want your dates and times as class "Date" or classes "POSIXct","POSIXlt". 
