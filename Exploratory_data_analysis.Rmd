---
title: "Exploratory Data Analysis"
author: "Nathan Young"
output: pdf_document
---
In this document, I'm taking notes based off the Coursera course Exploratory Data Analysis. 

## Week 1
### Setting Your Working Directory (Windows)
To find current working directory: getwd().

The working directory is where R finds all of its files for reading and writing on your computer. When you read data or write things out using functions or write.csv, they will be read or written to your home, your working directory. 

To find files in current working directory: dir()

It is a good idea when working on a class or project to create a folder that you store everything in for the project and set this folder as the working directory. 

To find what is in workspace: ls()

If a R file is saved in the working directory (visible when running dir()), you can source it to load its contents to the workspace with source("filename")

### Principles of Analytic Graphics
Based on book by Edward Tuffey, Beautiful Evidence, with the following principles. 

1. Show Comparisons
  + Evidence for a hypothesis is always *relative* to another competing hypothesis
  + Always ask "compared to what?" Evidence is relative

In boxplot example of an air cleaner vs change in asthma symptom free days, what are we comparing to? We need to compare to a control (or something eise) so we can see the impact of the treatment. 

2. Show causality, mechanism, explanation, systematic structure. 
  + What is your causal framework for thinking about a question? 

In prior example of air cleaner and asthma days, what improves their symptoms? Hypothesize that air cleaner is removing particulate matter and improving symptoms. So we might also show a plot of fine particulate matter in control vs air cleaner. This allows us to say that not only do symptoms decrease but the air is cleaner linking the particulate matter and symptoms.  

3. Show multivariate data
  + multivariate means more than two variables. 
  + The real world is multivariate
  + Need to "escape flatland"

This rule boils down to show as much data on a single plot as you can. And the reason is because the world is inherently multivariate, including more information on a plot better mirrors the world. 

Example plot of concentration of particulate matter vs daily mortality in NYC from 1987 till 2000. Overall slight negative association between PM10 levels and mortality. Regression line slopes downward slightly. Could hypothesize that lower airpollution is associated with higher mortality but if you include additional confounding variables such as season it can change the picture. When broken down to each season, there is now a slight positive correlation (simpson's paradox)

4. Integration of evidence
  + Completely integrate words, numbers, images, and diagrams
  + Data graphics should make use of many modes of data presentation
  + Don't let hte tool drive the analysis

Use as many different modes of evidence or displaying evidence as you can. Don't show only a plot, or a table, but use multiple to make the information as rich as possible. Advantage of R, use flexible tools. 

5. Describe and document the evidence with appropriate labels, scales, sources, etc. 
  + A data graphic should tell a complete story that is credible. 

Preserve the code that made the plot and the souces of where you got the data to maintain the credibility. 

6. Content is king
  + Analytical presentation ultimately stand or fall depending on the quality, relevance, and integrity of their content. 

What's the story, what's the data, whats the best way to present it? 

### Exploratory Graphics
How to construct graphs for yourself so you can look at the data and explore what's going on in data ses youre looking at. 

Why do we use graphs in data analysis? 

* To understand data properties
* To find patterns in data
* To suggest modeling strategies
* To 'debug' analyses
* To communicate results

Exploratory graphs are used for the first 4. 

Characteristic of exploratory graphs. 

* Made quickly
* A large number are made
* Goal is for personal understanding
* Axes/legends are generally cleaned up later
* Color/size primarily used for information

Example with real data set on pollution. Question: are there any counties in the US that exceed the standard of 12ug/m3 for fine particle pollution? Downloaded and cleaned already for example. 

Simple summaries of data:

* One dimension: five-number summary, boxplots, histograms, density plot, barplot. 

summary() produces a 5 number summary of min, 1st quartile, median, mean, 3rd quartile, and max. 
boxplot(data, col = "blue") will creaet a boxplot
abline(h = y) creates a horizontal line on boxplot
hist(data, col = "green") for a histogram
rug(data) plots points under the histogram, helps see outliers. 
Can change breaks on a histogram with breaks = n. Don't want bins to be too big or else its noisy, or too small or you can't see shape. 
Can put abline on histogram too, example has two: one at max of 12 and another at median. 
barplot() can show a graphical summary for categorical data. For example plotting over region shows number of counties in the west and east.

* Two dimension: multiple/overlayed 1D plots with lattice/ggplot2, scatterplots, smooth scatterplots, overlayed/multiple 2d plots and coplots, color/size/shape to add dimension, spinning plots, actual 3d plots (not as useful). 

Multiple boxplots:
```{r, eval = FALSE}
boxplot(pm25 ~ region, data = pollution, col = "red")
```
Multiple histograms: 
```{r, eval = FALSE}
par(mfrow(c = (2,1), mar = c(4, 4, 2, 1))
hist(subset(pollution, region == "east")$pm25, col = "green")
hist(subset(pollution, region == "west")$pm25, col = "green")
```
Scatterplot: 
```{r, eval = FALSE}
with(pollution, plot(latitute, pm25))
abline(h = 12, lwd = 2, lty = 2)
```
Scatterplot with color: 
```{r, eval = FALSE}
with(pollution, plot(latitude, pm25, col = region))
abline(h = 12, lwd = 2, lty = 2)
```
Multiple scatterplots: 
```{r, eval = FALSE}
par(mfrow = c(1,2), mar = c(5, 4, 2, 1))
with(subset(pollution, region == "west"), plot(latitude, pm25, main = "West"))
with(subset(pollution, region == "east"), plot(latitude, pm25, main = "East"))
```

Summary:

* Exploratory plots are "quick and dirty"
* Let you summarize the data (usually graphically) and highlight any broad features
* Explore basic questions and hypotheses (and perhaps rule them out)
* Suggest modeling strategies for the "next step"

## Plotting Systems in R
### The Base Plotting System

* The oldest plotting system in R
* "Artist's palette" model
* Start with blank canvas and build up from there
* Start with plot function (or similar)
* Use annotation functions to add/modify (text, lines, points, axis)
* Convenient, mirrors how we think about building plots and analyzing data. 
* Can't go back once plot has started (ie to adjust margins); need to plan in advance. 
* Difficult to "translate" to others once a new plot has been created (no graphical "language")
* Plot is just a seris of R commands so you have to set and control everything very carefully. 

```{r}
library(datasets)
data(cars)
with(cars, plot(speed, dist))
```

### The Lattice System

* Very different school of thought from base plotting system
* Installed with package
* Plots are created with a single function call (xyplot, bwplot, etc) and require a lot of information. 
* Most useful for conditioning/coplots types of plots: Looking at how y changes with x across levels of z. Can also be called panel plots. 
* Things like margins/spacing set automatically because entire plot is specified at once. 
* Good for putting many plots on a screen. 
* Downsides. Sometimes awkward to specify an entire plot in a single function call. 
* Annotation in plot is not especially intuitive. 
* Use of panel functions and subscripts difficult to wield and requires intense preparation. 
* Cannot "add" to the plot once it is created. 

```{r}
library(lattice)
state <- data.frame(state.x77, region = state.region)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4,1))
```

### The ggplot2 System

* Comes from the grammar of graphics
* Splits the difference between base and lattice in a number of ways 
* Automatically deals with spacings, text, tiles but also allows you to annotate by "adding" to a plot
* Superficial similarity to lattice but generally easier/more intuitive to use
* Default mode makes many choices for you (but you can still customize to your heart's desire)
* Useful for conditioning plots as well. 

```{r}
library(ggplot2)
data(mpg)
qplot(displ, hwy, data = mpg)
```

## Base Plotting System (part 1)
### Plotting System
The core plotting and graphics engine in R is encapsulated in the following packages: 

* *graphics*: contains plotting functions for the "base" graphing system, including plot, hist, boxplot, and many others
* *grDevices*: contains all the code implementing the various graphics devices including X11, PDF, PostScript, PNG, etc. 

The lattice plotting system is implemented in the following packages: 

* *lattice*: contains code for producing Trellis graphics, which are independent of the "base" graphics system; includes functions like xyplot, bwplot, levelplot. 
* *grid*: implements a different graphing system independent of the "base" system; the *lattice* package builds on top of *grid*; we seldom call from *grid* directly. 

### The process of making a plot
WHen making a plot one must first make a few considerations

* Where will the plot be made? On screen? In a file? 
* How will the plot be used? 
  + Is the plot for viewing temporarily on screen?
  + WIll it be presented in a web browser? 
  + Will it eventually end up in a paper that might be printed? 
  + Are you using it in a presentation? 
* Is there a large amount of data going into the plot? Or just a few points? 
* Do you need to be able to resize? 
* What graphics system do you want to use? base, lattice, ggplot2? Generally cannot be mixed. 

We focus on base plotting for screen device in this set of notes. 

Two phases: 

* Initialize a new plot
* Annotate a plot

* calling plot(x,y) or hist(x) will launch a graphics device and draw a new plot on the device. 
* If the arguments to plot are not some special cases, then the default method is called; it has many arguments letting you set title, labels, etc. 
* base graphics system has many parameters that can be set and tweaked and documented in ?par, could be helpful to memorize. 

Histogram
```{r}
library(datasets)
hist(airquality$Ozone) # draw a new plot
```

Scatterplot
```{r}
with(airquality, plot(Wind,Ozone))
```

Boxplot
```{r}
airquality <- transform(airquality, Month = factor(Month))
boxplot(Ozone ~ Month, airquality, xlab = "Month",ylab = "Ozone (ppb)")
```

Important base graphics parameters

* pch: plotting symbol (default open circle). Can take a number or a character "A"
* lty: line type (default solid line), can be dashed, dotted, etc. 
* lwd: line width; specified as an integer multiple
* col: plotting color, specified as a number, string, or hex code; colors() function gives a vector of colors by name
* xlab: character string for x-axis label
* ylab: character string for y-axis label
* par(): used to specify *global* graphics parameters that affect all plots in an R session. Can be overridden when specified as arguments to specific plotting functions. 
* las: orientation of the axis labels on the plot. 
* bg: background color
* mar: Margin size
* oma: outer margin size
* mfrow: number of plots per row, column (plots are filled row-wise)
* mfcol: number of plots per row, column (plots are filled column-wise)

Can see defaults by calling par() function with the name. For example 

```{r}
par("lty")
```

### Base Plotting Functions

* **plot**: make a scatterplot, or other type of plot depending on the class of the object being plotted. 
* **lines**: add lines to a plot, given a vector x values and a corresponding vector of y values (or a 2-column matrix); this function just connects the dots. 
* **points**: add points to a plot
* **text**: add text labels to a plot using specified x,y coordinates
* **title**: add annotations to x, y axis labels, title, subtitle, outer margin
* **mtext**: add arbitrary text to the margins (inner or outer) of the plot
* **axis**: adding axis ticks/labels

Base plot with annotation: 

```{r}
library(datasets)
with(airquality, plot(Wind, Ozone))
title(main = "Ozone and Wind in New York City") # Add a title
```

```{r}
# Add color to one subset
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York city"))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
```

```{r}
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City", type = "n")) # type = n initialized the plot but doesn't plot the data 
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
with(subset(airquality, Month != 5), points(Wind, Ozone, col = "red"))
legend("topright", pch = 1, col = c("blue","red"), legend = c("May", "Other Months"))
```

With regression line:  
```{r}
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City", pch = 20))
model <- lm(Ozone ~ Wind, airquality)
abline(model, lwd = 2)
```

Multiple Plots. 
```{r}
par(mfrow = c(1,2))
with(airquality, {
  plot(Wind, Ozone, main = "Ozone and Wind")
  plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
})
```

Label over entire panel
```{r}
par(mfrow = c(1,3), mar = c(4,4,2,1), oma = c(0,0,2,0))
with(airquality, {
  plot(Wind, Ozone, main = "Ozone and Wind")
  plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
  plot(Temp, Ozone, main = "Ozone and Temperature")
  mtext("Ozone and Weather in New York City", outer = TRUE)
})
```

### Summary

* Plots in the base plotting system ae created by calling successive R functions to "build up" a plot
* Plotting occurs in two states: 
  + Creation of a plot
  + Annotation of a plot (adding lines, points, text, legends)
* The base plotting system is very flexible and offers a high degree of control over plotting. 

## Base Plotting Demonstration
x <- rnorm(100)
hist(x)
y <- rnorm(100)
plot(x,y)
z <- rnorm(100)
plot(x,z)
Note the margins can be adjusted
par(mar = c(2, 2, 2, 2))
plot(x,y)
par(mar = c(4,4,2,2))
plot(x,y)
Change plotting symbols
plot(x,y, pch = 20)
To see examples: 
example(points)
title("Scatterplot")
text(-2, -2, "label")
legend("topleft", legend = "legend")
fit <- lm(y ~ x)
abline(fit, lwd = 3, col = "blue")
plot(x,y,xlab = "Weight", ylab = "Height", main = "Scatterplot")
legend("topright", legend = "Date", pch = 20)
z <- rpois(100, 2)
par(mfrow = c(2,1))
plot(x,y,pch = 20)
plot(x,z,pch = 19)
par("mar")
par(mar = c(2,2,1,1)
plot(x,y,pch = 20)
plot(x,z,pch = 19)
Annotate a plot
par(mfrow = c(1,1))
x <- rnorm(100)
y <- x + rnorm(100)
g <- gl(2,50)
g <- gl(2, 50, labels = c("Male", "Female"))
plot(x,y, type = "n") # Blank screen, we'll add data in additional layers
points(x[g == "Male"], y[g == "Female"], col = "green")
points(x[g == "Female"], y[g == "Male"], col = "blue")

## Graphics Devices in R
What's a graphics device? 

* Something where you can make a plot appear
  + A window on your computer screen (screen device)
  + A PDF file (file device)
  + A PNG of JPG file (file device)
  + A scalable vector graphics (SVG) file (file device)
* When you make a plot in R, it has to be "sent" to a specific graphics device
* The most common place for a plot to be "sent" is the *screen device*
  + On a Mac the screen device is launched with `quartz()`
  + On windows, it is launched with `windows()`
  + On Unix/Linux, it is launched with `x11`
* When making a plot, you need to consider how the plot will be used to determine what device the plot should be sent to. The list of devices is found in `?Devices`; also others created by users on CRAN
* For quick visualization and exploratory analysis, usually you want to use the screen device. 
  + FUnctions like `plot` in bas, `xyplot` in lattice, or `qplot` in ggplot2 will default to sending a plot to the screen device. 
  + On a given platform, there is only one screen device. 
* For plots that may be printed out or be incorporated into a document, usually a *file device* is more appropriate. Many options. 

How does a plot get created? 
Two apporaches: 

1. Call a plotting function
2. The plot appears on the screen device
3. Annotate if necessary
4. Enjoy

Commonly used for file devices: 

1. Explicitly launch a graphics device
2. Call a plotting function to make a plot (none will appear on screen)
3. Annotate plot with necessary
4. Explicitly close graphics device with `dev.off()` (very important) 

```{r, eval = FALSE}
pdf(file = "myplot.pdf") # open pdf device in WD
with(faithful, plot(eruptions, waiting))
title(main = "Old Faithful Geyser Data")
dev.off()
```

Graphics File Devices
Two basic types: vector or bitmap. 
Vector formats, not as useful for large amounts of data as each object is a chunk of memory. 

* pdf: useful for line-type graphics, resizes well, portable, not efficient if plot has many objects/points
* svg: XML based scalable vector graphics; supports animation and interactivity, potentially useful for web-based plots
* win.metafile: Windows metafile format
* postscript: older format, also resizes well, usually portable, can be used to create encapsulated postscript files; Windows systems often don't have a postscript viewer. 
Bitmap formats: in general don't resize well after being plotted

* png: bitmapped format, good for line drawings or images with solid colors, uses lossless compression, most web browsers can read natively, good for plotting many points, does not resize well. 
* jpeg: good for photographs or natural scenes, uses lossy compression, good for plotting many many many points, does not resize well, can be read by almost any web browser, not great for line drawings
* tiff: creates bitmap files in TIFF format, supports lossless compression
* bpm: windows native bitmapped format

### Multiple open graphics devices: 
Possible to open multiple devices, for example when viewing multiple plots at once. Can only plot on one graphics device at a time. The currently active device can be found with `dev.cur()`. Every open device is assigned an integer of 2 or greater. Can change active device with `dev.set(<integer>)`

### Copying plots
Can copy a plot to another device, saving retyping of code. `dev.copy()` from one device to another. `dev.copy2pdf()` specifically copies to pdf. Note, it isn't an exact operation and can give slightly different results. Example: `dev.copy(png, file = "geyserplot.png"); dev.off()`  

### Summary

* Plots must be created on a graphics device.  
* Default graphics device is almost always screen device, most useful for exploratory analysis. 
* File devices are useful for creating plots to be used in other documents or sent to others
* For file devices: 
  + Vector formats are good for line drawings and plots with solid colors using a modest number of points
  + bitmap formats are good for plots with a large number of points, natural scenes or web-based plots

## Week 2
### Lattice Plotting System
Often used for high dimension data and making many plots at once.  

* Implemented in the *lattice* package which contains code for producing Trellis graphics which are independent of the base system. Includes functions like **xyplot**, **bwplot**, **levelplot**. 
* *grid*: implements a different graphing system independent of base system, *lattice* builds on top of grid. Grid is seldom called directly. 
* all plotting and annotation is done at once. 

Main functions: 

* `xyplot`: scatterplots
* `bwplot`: box and whiskers
* `histogram`: histogram
* `stripplot`: like boxplot, but with points
* `dotplot`: plot dots on "violin strings"
* `splom`: scatterplot matrix; like `pairs` in base
* `levelplot, contourplot`: for plotting image data

In general, lattice functions take a formula for the argument. `xyplot(y ~ x | f * g, data)`.

* Use formula notation, hence the ~
* On the left of ~ is the y-axis variable, right is x-axis variable
* f and g are optional conditioning variables and * indicates an interaction between the two. 
* Second argument is the data frame or list from which variables should be looked up. 
* If no data frame or list is passed, parent frame is used. If no other arguments are passed, defaults can be used. 

Simple lattice plot

```{r}
library(lattice)
library(datasets)
xyplot(Ozone ~ Wind, data = airquality)

# Convert Month to a factor variable
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5,1))
```

Lattice Behavior differs from base in one critical way: 

* Base plots directly to graphics device
* Lattice return an object of class **trellis**
* Print methods for lattice functions do work of plotting the data on graphics device. 
* Lattice functions return "plot objects" that in principle can be stored (usually better to just save code and data). 
* On command line, trellis objects are auto-printed so it appears the function is plotting the data. 

```{r}
p <- xyplot(Ozone ~ Wind, data = airquality) ## Nothing happens
print(p) ## Plot appears
```

Lattice Panel Functions

* Lattice functions have a **panel function** which controls what happens inside each panel of the plot
* The *lattice* package comes with default panel functions, but you can supply your own if you want to control what happens in each panel. 
* Panel functions receive the x/y coordinate of the data points in their panel along with any optional arguments. 

```{r}
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
y <- x + f - f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c("Group 1", "Group 2"))
xyplot(y ~ x | f, layout = c(2,1))

# Custom panel function
xyplot(x ~ y | f, panel = function(x,y,...) {
  panel.xyplot(x,y,...) # First call of the default panel function for 'xyplot'
  panel.abline(h = median(y), lty = 2) # add a horizontal line at the median
})

# Regression line
xyplot(x ~ y | f, panel = function(x, y, ...) {
  panel.xyplot(x, y, ...) 
  panel.lmline(x, y, col = 2)
})
```

Summary: 

* lattice plots are constructed with a single function call to a core lattice function such as `xyplot`
* aspects like margins and spacing are automatically handled and the defaults are usually sufficient
* the lattice system is ideal for creating conditioning plots where you examine the same kind of plot under many different conditions
* panel functions can be specified/customized to modify what is plotted in each of the plot panels. 

## ggplot2 Plotting System

What is ggplot2? A package in R implementation of the *Grammar of Graphics* that is conceptualized as a grammar where different 'nouns' and 'verbs' can be combined to create a whole unique system. It is a 'third' graphics system built on grid.  
The grammar of graphics represents and abstraction of graphics ideas/objects. Think "verb", "noun", "adjective" for graphics. Allows for a "theory" of graphics on which to build new graphics and graphics objects. Meant to "shorten the distance from mind to page."  
From ggplot2 book: "In brief, the grammar tells us that a statistical graphic is a **mapping** from data to **aesthetic** attributes (color, shape, size) of **geometric** objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system."  

Basics: `qplot()`

* Works much like the `plot` function in base graphics. 
* Looks for data in a data frame, similar to lattice, or in the parent environment. Make sure data frame is organized! 
* Plots are made up of *aesthetics* (size, shape, color) and *geoms* (points and lines). 
* Factors are important for indicating subsets of the data (if they are to have different properties); they should be **labeled**
* The `qplot` hides what goes on underneath, which is okay for most operations. 
* `ggplot` is the core function and is very flexilble for doing things `qplot` cannot do. 

Examples: mpg dataset
```{r}
library(ggplot2)
# Notice in str that the factor labels are appropriately labeled!
str(mpg)

# The "hello world" plot
qplot(displ, hwy, data = mpg) #specified data frame

# Modifying with different aesthetics. qplot does legend and color automatically
qplot(displ, hwy, data = mpg, color = drv)

# adding a statistic. A smoothing "loess"
qplot(displ, hwy, data = mpg, geom = c("point", "smooth"))

# Histogram with qplot, 
qplot(hwy, data = mpg, fill = drv)

# Facets: variable on left (rows) and right (columns) separated by ~
qplot(disp, hwy, data = mpg, facets = .~drv)

qplot(hwy, data = mpg, facets = drv~., binwidth = 2)
```

MAACS Cohort
Mouse allergen and asthma cohort study; data not currently available. 
Things highlighted: 

* density geom: `qplot(x, data, geom = "density")`
* shape: `qplot(shape = factor variable)`
* smooth with other model: `qplot(initialize) + geom_smooth(method = "lm")`

Summary of `qplot`

* `qplot()` is analog to `plot()` but with many build in features
* Syntax somewhere in between base/lattice
* Produces very nice graphics, essentially publication ready (if you like the design)
* Difficult to go against the grain and customize (don't bother, use ggplot2 power in that case)

### `ggplot`
Basic components of `ggplot2` graphic. 

* A data frame
* **aesthetic mapping**: how data is mapped to color, size, etc
* **geoms**: geometric objects like points, lines, shapes
* **facets**: for conditional plots
* **stats**: statistical transformations like binning, quantiles, smoothing
* **scales**: what scale an aesthetic map uses (example: male = red, female = blue)
* **coordinate system**

Building plots with ggplot2: 

* When building, the "artist's palette" model is closest analogy similar to plot
* Plots are built in layers
  + Plot the data
  + Overlay a summary
  + Metadata and annotation

Uses mouse allergen study again (data not available :( )

* Start with call to ggplot: `g <- ggplot(data, aes(x,y))`
* Can see summary of this: `summary(g)`
* Explicitly save and print ggplot object `p <- g + geom_point(); print(p)`
* Don't have to save in prior step, but can be useful for saving typing. 
* Add summaries: `+geom_smooth()` default is "loess", or `+geom_smooth(method = "lm")
* Can add facets: ` + facet_grid(. ~ factor variable)` puts facets as one row and multiple columns. 
  + Easier to have the metadata names etc be already appropriate as ggplot will just take these as is and you don't have to respecify. 

Annotation: 

* Labels: xlab(), ylab(), labs(), ggtitle()
* Each of the "geom" functions has options to modify
* For things that only make sense globally, use theme()
  + `theme(legend.position = "none")`
* Two standard appearance themes are included
  + theme_gray(): default
  + theme_bw(): most stark/plain

Modifying aesthetics: 

* color of points: `geom_point(color = "steel blue", size = 4, alpha = 1/2)`
  + Since these are constants, they don't need to be specified in aes()
* color of points with factor var: `geom_point(aes(color = factor var), size = 4, alpha = 1/2)`
* modify labels: `labs(title = "TITLE") + labs(x = expression("log")) + labs(y = "Y")`
* Customize smoohther: `geom_smooth(size = 4, linetype = 3, method = "lm", se = FALSE)`
* Customize theme: change background to bw and font `theme_bw(base_family = "Times")`

A note about axis limits and outliers
If you change the y limits to be within a range as `g + geom_line() + ylim(-3, 3)`, ggplot will actually subset the data and not show outliers at all. Alternatively, you can use `g + geom_line() + coord_cartesian(ylim = c(-3,3))`. This will include the point and show the vertical lines going off the edge of he plot. 

Adding complexity: How does relationship between two variables vary with two more variables? Will need to change a continuous variable to categorical with a reasonable set of ranges. 

Making Tertiles: 

1. Calculate deciles `cutpoints <- quantile(data, seq(0, 1, length = 4)`
  + creates cutpoints for data with quartiles defined by sequence 0, .3, .6, 1
2. Cut data at deciles and create new factor variable `deciles <- cut(data, cutpoints)`
4. See levels of new factor variable `levels(deciles)`

Code for a final, more complicated plot: 
```{r, eval = FALSE}
g <- ggplot(data, aes(x, y))
# add layers

g + geom_point(alpha = 1/3) # scatterplot with partially transparent points
  + facet_wrap(categorical1 ~ categorical2, nrow = 2, ncol = 4) #panels with two different conditioning variables. 
  + geom_smooth(method = "lm", se = FALSE, col = "steelblue")
  + theme_bw(base_family = Avenir", base_size = 10)
  + labs(x = expression("log " * PM[2.5])
  + labs(y = "Nocturnal Symptoms")
  + labs(title = "MAACS Cohort")
  ```
  
## Week 4
### Heirarchical Clustering
A 'bread and butter technique' when it comes to visualizing high dimensional or multidimensional data.  
Can we find things that are close together? Clustering organizes things that are **close** into groups. How do we define close? How do we group things? How do we visualize groups? How do we interpret the grouping?  
Cluster analysis is important and widely used in science and other applications.  
Heirarchical Clustering: 

* An agglomerative approach
  + Find closest two things
  + Put them together
  + Find next closest

Requires a defined distance and a merging approach and produces a tree showing how close things are to each other.  
Arguable the most important is how we define close. GIGO.  
Distance or similarity? 

* Continuous - euclidean distance. extends nicely to higher dimensional problems
* Continuous - correlational similarity
* Binary - manhattan ditance; think about walking down blocks; sum of absolute distances in each direction. 

Pick one that makes sense for your problem. 

Example: 
```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each = 4, sd = 0.2))
plot(x,y,col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```

To run a heirarchical clustering model, you first need to calculate pairwise distances with `dist`

```{r}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame) # Euclidean default. 
hClustering <- hclust(distxy)
plot(hClustering)
```

As you move up the dendogram, it shows which things were clustered first. Clustering goes by pairing the closest points to a new cluster you can think of as a super point. The clustering continues till every data point is combined in the tree. If we want, we can cut the tree at different horizontal lines to get different numbers of clusters from the extreme of one giant cluster to all the original data points. 

Prettier Dendrograms
It is possible to have prettier dendograms that can be created with custom functions. Things like labels and colors of the leaves.  
The next question is how do you merge the points? One is called average linkage, where the new coordinate is the average of the x and y coordinates. Can also use "complete linkage" 

Last function: `heatmap`
Useful for large datasets that are roughly on the same scale, a heatmap can help get a quick look. Essentially runs a heirarchical analysis on the rows and columns

```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)
```

Rows of the table are observations and columns are sets of observations. heatmap uses clustering to organize the observations. Reorders the rows and columns according to the heirarchical ordering. This shows that the x values have three rough groups. 

Notes and further resources: 

* Gives an idea of hte relationships between variables/observations
* Picture may be unstable
  + Change a few points
  + Have different missing values
  + Pick a different distance
  + Change the merging strategy
  + Change the scale of points for one variable
* But it is deterministic
* Choosing where to cut isn't always obvious
* Should be primarily used for exploration
* Rafa's Distances and Clustering Video
* Elements of statistical learning

## K-Means Clustering
Basic principle: can we find things that are close together? 

* How do we define close? Most important: GIGO. 
  + Distance or similarity: continuous euclidean distance, continuous correlation similarity, binary manhattan distance. Pick one that make sense for the problem. 
* How do we group things? 
* How do we visualize the grouping?
* How do we interpret the grouping?

K-means clustering is a way of partitioning data into a given number of clusters. 

* A partitioning approach
  + Fix a number of clusters
  + Get "centroids" of each cluster
  + Assign things to closest centroid
  + Recalculate centroids
* Requires: a defined distance metric, a number of clusters, an initial guess as to cluster centroids. 
* Produces: final estimate of cluster centroids, an assignment of each point to clusters. 

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```

Can see three clusters above. K-means starts by grouping the data to the different centroids passed in then recalculates the centroids and reassign the values. `kmeans` function in R. Important parameters are `x`, `centers`, `iter.max`, `nstart`

```{r}
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
kmeansObj$cluster # shows which points are in which cluster

# plot: 
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
```

Can also use heatmaps. 
```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix:1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```

Notes and further resources: 

* K-means requires a number of clusters, picked by eye/intuition or by cross validation/information theory etc. 
* K-means is not deterministic, different # of clusters and different # of iterations. Useful to run a few times to make sure you have a stable solution point. 
* Rafael Irizarry's Distances and Clustering Video
* Elements of statistical learning. 

### Dimension Reducing
Suppose we have some matrix data: 

```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

# Run heirarchical cluster analysis
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

Add a pattern: 
```{r}
set.seed(678910)
for (i in 1:40) {
  # flip a coin
  coinFlip <- rbinom(1, size = 1, prob = 0.5)
  # if a coin is heads, add a common pattern to that row
  if (coinFlip) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
  }
}

# some of the rows now have a pattern with a mean of 3
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

# Cluster analysis is able to find the two clusters for the columns
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

# Patterns in rows and columns
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, , xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "column", ylab = "Column Mean", pch = 19)

```

Related problems
You have multivariate variables X1, ..., Xn so X1 = (X11, ... X1m)

* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible. 
* If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data. 

The first goal is **statistical** and the second goal is **data compression**

Related solutions
Single value decomposition (SVD)

* If X is a matrix with each variable in a column and each observationin a row then the SVD is a "matrix decomposition" X = UDV^T where the columns of U are orthogonal (left singular vectors), the columns of V are orthogonal (right singular vectors) and D is a diagonal matrix (singular values). 

Principal Components Analysis (PCA)
The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables. 

### Expanding the SVD
Finding the components of the SVD: u and v

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, , xlab = "Row", ylab = "First left singular vector", pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
```

The SVD components here show a clear pattern that the SVD was able to illustrate without any input from the user. 

Next component of the SVD: the variance explained
You can think of each element of the diagonal matrix as the amount of variance that explained by that element. Typically arranged that the first explains the most, the next less, etc. 

```{r}
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```

Show that the relationship of the SVD to principal components is close. 

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = "Principal Component 1", ylab = "Right Singular Vector 1")
abline(c(0, 1))
```

Closer look at the variance explained. With a matrix of 0 and 1, first 5 columns are 0 and second 5 are all 1. Note that the first singular value explains 100% of the variance of the dataset. The basic matrix here, even though it has 10 columns and 40 rows, the only relevant piece is are you in columns 1-5 or 6-10 so the whole thing is explained with one dimension.  

```{r}
constantMatrix <- dataMatrixOrdered * 0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1), each = 5)}
svd1 <- svd(constantMatrix)
par(mfrow = c(1, 3))
image(t(constantMatrix)[, nrow(constantMatrix):1])
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```

What if we add a second pattern?

```{r}
set.seed(678910)
for (i in 1:40) {
  # flip a coin
  coinFlip1 <- rbinom(1, size = 1, prob = 0.5)
  coinFlip2 <- rbinom(1, size = 1, prob = 0.5)
  # If coin is heads add a common pattern to that row
  if (coinFlip1) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,5), each = 5)
  }
  if (coinFlip2) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), 5)
  }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]

# This display shows the true underlying pattern we generated.
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column", ylab = "Pattern 2")
```

The question is, given this data can we have an algorith that can detect this underlying pattern without our prior knowledge? That's what the svd is for

```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")
```

Since we know the true pattern, it is easier to parse the first and second vector patterns as being indicative of what we know is already there. The problem we have is that they are confounded together and are difficult to separate. 

d and variance explained. The first variable explains over 50% of the variation in the dataset. 

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular Value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance Explained", pch = 19)
```

A problem with SVD is missing values. 

```{r}
dataMatrix2 <- dataMatrixOrdered
## Randomly insert some missing data
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
svd1 <- svd(scale(dataMatrix2)) # doesn't work! 
```

We need to take care of the missing values! One way is with imputing via `impute`

```{r, eval = FALSE}
library(impute) #available from http://bioconductor.org
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
# takes missing value and imputes it with k nearest neighbors
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(dataMatrix2))
par(mfrow = c(1, 2)); plot(svd1$v[, 1], pch = 19); plot(svd2$v[, 1], pch = 19)
```

Example with a face datafile:  
```{r, eval = FALSE}
load("data/face.rda")
image(t(faceData)[, nrow(faceData):1])
# Lets look at variance explained
svd1 <- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = "Singular Vector", ylab = "Variance Explained")
```
The first 5-10 singular vectors capture most of the image so we can create an approximation of the image using these. 

```{r, eval = FALSE}
svd1 <- svd(scale(faceData))
# Note %*% is matrix multiplication

# Here svd1d[1] is a constant
approx1 <- svd1$u[, 1] %*% t(svd1$v[, 1]) * svd1$d[1]

# In these examples, we need to make the diagonal matrix out of d
approx5 <- svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[, 1:5])
approx10 <- svd1$u[, 1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[, 1:10])
```

Data compression and summarizing a dataset are two sides of the same coin. 

Notes and further resources:

* Scale of data matters, needs to be on same scale. 
* PC's/SV's may mix real patterns
* Can be computationally intensive
* Advanced data analysis from an elementary point of view
* Elements of statistical learning
* Alternatives:
  + Factor analysis
  + Independent components analysis
  + Latent semantic analysis

## Working with color
How to specify different colors using different palettes. Not as important as the data but can help make relationships clear. 

* The default color schemes for most plots in R are not great. 
* Recently there have been developments to improve the handling/specification of colors in plots/graphics. 
* Functions in R and external packages that are useful

Col = 1, 2, 3 (black, red, green) is easy in R but these colors aren't ideal in all circumstances.  
Heat.colors and topo.colors are common as well.  

Color Utilities in R: 

* The **grDevices** package has two functions: `colorRamp` and `colorRampPalette`
* These functions take palettes of collors and halp to interpolate between the colors. 
* The function `colors()` lists the names of colors you can use in any plotting function.  

* `colorRamp`: takes a palette of colors and returns a function that takes values between 0 and 1, indicating the extremes of the color palette
* `colorRampPalette`: takes a palette of colors and returns a function that takes integer arguments and returns a vector of colors interpolating the palette (like `heat.colors` or `topo.colors`). 

```{r}
# Example, imagine a painters palette with a spot of red and blue
pal <- colorRamp(c("red", "blue"))
pal(0) #retuns 255, 0, 0 (all red)
pal(1) # returns 0, 0, 255 (all blue)
pal(0.5) # returns 127.5, 0, 127.5 (half and half)
```

Can also pass the function a sequence: `pal(seq(0, 1, len = 10))` and you will get a series of 10 colors mixed in different proportions.  

colorRampPalette Example: 
```{r}
pal <- colorRampPalette(c("red", "yellow"))
pal(2) # will return two colors in hex for the extremes in this case
pal(10) # will return 10 colors in hex of an even spread from all red to all yellow. 
```

How do I come up with my own interesting palette of colors? You can always use what was done in prior examples, can also use `RColorBrewer` package. 

* Available on CRAN, contains interesting/useful color palettes for different types of data. 
* There are 3 types of palettes: sequential (ordered data), diverging (deviate from something such as mean), qualitative (representing non-ordered data such as factors). 
* Palette information can be used in conjunction with `colorRamp` and `colorRampPalette`.  

```{r, eval = FALSE}
library(RColorBrewer)
cols <- brewer.pal(3, "BuGn") # Name of palette not intuitive, look at help file
cols
pal <- colorRampPalette(cols)
image(volcano, col = pal(20))
```

Smoothscatter can also use colors. Useful if you have a LOT of data and don't want a bunch of dots just overlapping each other. `smoothScatter` creates a 2d histogram

```{r}
x <- rnorm(10000)
y <- rnorm(10000)
smoothScatter(x, y)
```

Other Plotting Notes: 

* The `rgb` function can be used to produce any color, returns a hex code. 
* Transparency can be added via the `alpha` parameter to `rgb`. 
* The **colorspace** package can be used for a different control over colors. 

```{r}
plot(x, y, col = rgb(0, 0, 0, 0.2), pch = 19)
```
